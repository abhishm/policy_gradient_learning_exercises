{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:gym.envs.registration:Making new env: FrozenLakeNonSlippery-v0\n",
      "[2016-09-01 09:55:38,771] Making new env: FrozenLakeNonSlippery-v0\n"
     ]
    }
   ],
   "source": [
    "import gym\n",
    "from gym.envs.registration import register, spec\n",
    "\n",
    "MY_ENV_NAME='FrozenLakeNonSlippery-v0'\n",
    "try:\n",
    "    spec(MY_ENV_NAME)\n",
    "except:\n",
    "    register(\n",
    "        id=MY_ENV_NAME,\n",
    "        entry_point='gym.envs.toy_text:FrozenLakeEnv',\n",
    "        kwargs={'map_name': '4x4', 'is_slippery': False},\n",
    "    )\n",
    "FLenv = gym.make(MY_ENV_NAME)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Given a policy, determine its action-value function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "import numpy as np\n",
    "from copy import deepcopy \n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "policy = defaultdict(lambda: np.ones(FLenv.action_space.n, dtype=float)/FLenv.action_space.n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def compute_action_value(policy = policy, env = FLenv, gamma = 1, \n",
    "                         max_episodes = 10, epsilon = 0.01):\n",
    "\n",
    "    q = defaultdict(list)\n",
    "    \n",
    "    error = np.inf\n",
    "    errors = []\n",
    "    for _ in xrange(max_episodes):\n",
    "        done = False\n",
    "        state = env.reset()\n",
    "        states = []\n",
    "        actions = []\n",
    "        rewards = []\n",
    "        returns = []\n",
    "        \n",
    "        while not done:\n",
    "            states.append(state)\n",
    "            \n",
    "            action = np.random.multinomial(1, policy[state]).argmax()\n",
    "            next_state, reward, done, _ = env.step(action)\n",
    "            state = next_state\n",
    "            \n",
    "            actions.append(action)\n",
    "            rewards.append(reward)\n",
    "            \n",
    "        return_so_far = 0\n",
    "        for reward in reversed(rewards):\n",
    "            return_so_far = reward + gamma * return_so_far\n",
    "            returns.append(return_so_far)\n",
    "        \n",
    "        # return is computed in reverse direction so correct the order\n",
    "        returns = returns[::-1]\n",
    "        \n",
    "        q_old = deepcopy(q)\n",
    "        \n",
    "        for state, action, return_so_far in zip(states, actions, returns):\n",
    "            q[(state, action)].append(return_so_far)\n",
    "            \n",
    "        error = np.max([abs(np.mean(value) - np.mean(q_old.get(key, 0)))\n",
    "                                  for key, value in q.items()])\n",
    "        \n",
    "        errors.append(error)\n",
    "        \n",
    "    q = {key:np.mean(value) for key, value in q.items()}\n",
    "    \n",
    "    return q #, errors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### How can we check that the implementation of compute_action_value is correct."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def compute_action_value_theoretically(policy = policy, behavior_policy = policy, env = FLenv, gamma = 1, \n",
    "                                       max_episodes = 10, epsilon = 0.01):\n",
    "    q = defaultdict(int)\n",
    "    \n",
    "    for _ in xrange(max_episodes):\n",
    "        done = False\n",
    "        state = env.reset()\n",
    "        \n",
    "        while not done:\n",
    "            action = np.random.multinomial(1, behavior_policy[state]).argmax()\n",
    "            next_state, reward, done, _ = env.step(action)\n",
    "            \n",
    "            q[state, action] = reward + gamma * (policy[next_state].dot([q[next_state, a] \n",
    "                                                                   for a in xrange(env.action_space.n)]))\n",
    "            \n",
    "            state = next_state\n",
    "    \n",
    "    return q        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "q_grid_world = compute_action_value(max_episodes=2000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "q_grid_world_theoretical = compute_action_value_theoretically(max_episodes=20000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### computing the difference between theoretical and Every-visit Monte Carlo Based Estimate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "diff_array = [abs(q_grid_world.get(key, 0) - value) for key, value in q_grid_world_theoretical.items()]\n",
    "diff_theo_prac_mean, diff_theo_prac_max  = np.mean(diff_array), np.max(diff_array)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The mean difference between the theoretical and practical estimate is 0.0128\n",
      "The max difference between the theoretical and practical estimate is 0.1488\n"
     ]
    }
   ],
   "source": [
    "print \"The mean difference between the theoretical and practical estimate is {:0.4f}\".format(diff_theo_prac_mean)\n",
    "print \"The max difference between the theoretical and practical estimate is {:0.4f}\".format(diff_theo_prac_max)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Implementation of retace ($\\lambda$)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def q_differences(q_1, q_2):\n",
    "    diff_array = [abs(q_1.get(key, 0) - value) for key, value in q_2.items()]\n",
    "    diff_mean, diff_max  = np.mean(diff_array), np.max(diff_array)\n",
    "    print \"The mean difference between the theoretical and practical estimate is {:0.4f}\".format(diff_mean)\n",
    "    print \"The max difference between the theoretical and practical estimate is {:0.4f}\".format(diff_max)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def compute_traces(states, actions, target_policy, behavior_policy, lambda_):\n",
    "    traces = []\n",
    "    \n",
    "    for state, action in zip(states, actions):\n",
    "        importace_sampling = target_policy[state][action]/behavior_policy[state][action]\n",
    "        trace = lambda_*min(importace_sampling, 1)\n",
    "        traces.append(trace)\n",
    "        \n",
    "    return traces"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def compute_targets(q, states, actions, rewards, gamma, target_policy, action_n):\n",
    "    deltas = []\n",
    "    for state, next_state, action, reward in zip(states[:-1], states[1:], actions[:-1], rewards[:-1]):\n",
    "        future_reward = target_policy[next_state].dot([q[next_state, a] for a in xrange(action_n)])\n",
    "        delta = reward + gamma * future_reward - q[state, action]\n",
    "        deltas.append(delta)\n",
    "    delta = rewards[-1] - q[states[-1], actions[-1]]\n",
    "    deltas.append(delta)\n",
    "    return deltas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def compute_one_n_step(gamma, targets, traces):\n",
    "    discount = 1\n",
    "    trace = 1\n",
    "    delta = 0\n",
    "    traces = traces + [1]\n",
    "    for i, target in enumerate(targets):\n",
    "        delta += discount*trace*target\n",
    "        discount *= gamma\n",
    "        trace *= traces[i]\n",
    "    return delta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def compute_n_step_target(q, states, actions, rewards, gamma, lambda_,\n",
    "                          target_policy, behavior_policy, action_n, step_n):\n",
    "    traces = compute_traces(states, actions, target_policy, behavior_policy, lambda_)\n",
    "    targets = compute_targets(q, states, actions, rewards, gamma, target_policy, action_n)\n",
    "    n_step_targets = []\n",
    "    for i in xrange(len(targets)):\n",
    "        target = compute_one_n_step(gamma, targets[i:i+step_n], traces[i+1:i+step_n])\n",
    "        n_step_targets.append(target)\n",
    "    return n_step_targets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def retrace(target_policy, behavior_policy, env, lambda_, step_n, alpha, gamma, max_episodes):\n",
    "    q = defaultdict(int)\n",
    "    action_n = env.action_space.n\n",
    "    \n",
    "    for itr in xrange(max_episodes):\n",
    "        done = False\n",
    "        state = env.reset()\n",
    "        states = []\n",
    "        actions = []\n",
    "        rewards = []\n",
    "        returns = []\n",
    "        \n",
    "        while not done:\n",
    "            states.append(state)\n",
    "            \n",
    "            action = np.random.multinomial(1, behavior_policy[state]).argmax()\n",
    "            next_state, reward, done, _ = env.step(action)\n",
    "            state = next_state\n",
    "            \n",
    "            actions.append(action)\n",
    "            rewards.append(reward)\n",
    "            \n",
    "        n_step_corrections = compute_n_step_target(deepcopy(q), states, actions, rewards, gamma, lambda_, target_policy, \n",
    "                                                   behavior_policy, action_n, step_n) \n",
    "                \n",
    "        for state, action, correction in zip(states, actions, n_step_corrections):\n",
    "            q[(state, action)] += alpha*correction\n",
    "            \n",
    "        #alpha /= (itr +1 ) ** 0.8\n",
    "        \n",
    "    return q\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Assertions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "t_policy = {0 : np.array([0.4, 0.6]), 1 : np.array([0.6, 0.4])}\n",
    "b_policy = {0 : np.array([0.6, 0.4]), 1 : np.array([0.4, 0.6])}\n",
    "q = {(0, 0) : 1, \n",
    "     (0, 1) : 2,\n",
    "     (1, 0) : 3,\n",
    "     (1, 1) : 4}\n",
    "\n",
    "# Asserting compute_traces\n",
    "assert(compute_traces([0, 0], [0, 0], t_policy, t_policy, 1) == [1, 1])\n",
    "assert(np.allclose(compute_traces([0, 0], [0, 0], t_policy, b_policy, 1), [2/3., 2/3.]))\n",
    "assert(np.allclose(compute_traces([0, 0], [0, 1], t_policy, b_policy, 1), [2/3., 1.]))\n",
    "assert(np.allclose(compute_traces([1, 0], [0, 1], t_policy, b_policy, 1), [1., 1.]))\n",
    "\n",
    "# Asserting compute_targets\n",
    "assert(np.allclose(compute_targets(q, [0, 0], [0, 0], [0, 0], 1, t_policy, 2), [0.6, -1]))\n",
    "assert(np.allclose(compute_targets(q, [0, 1], [0, 0], [0, 0], 1, t_policy, 2), [2.4, -3]))\n",
    "\n",
    "# Asserting compute_one_step\n",
    "assert(np.allclose(compute_one_n_step(1, [1, 2, 3], [1, 2]), 9))\n",
    "assert(np.allclose(compute_one_n_step(0.1, [1, 2, 3], [1, 2]), 1.26))\n",
    "\n",
    "# Asserting compute_n_step_target\n",
    "assert(np.allclose(compute_n_step_target(q, [0, 0], [0, 0], [0, 0], 1, 1, t_policy, b_policy, 2, 1), \n",
    "                   compute_targets(q, [0, 0], [0, 0], [0, 0], 1, t_policy, 2)))\n",
    "tmp_traces = compute_traces([0, 0], [0, 0], t_policy, b_policy, 1)\n",
    "tmp_targets = compute_targets(q, [0, 0], [0, 0], [0, 0], 1, t_policy, 2)\n",
    "assert(np.allclose(compute_n_step_target(q, [0, 0], [0, 0], [0, 0], 1, 1, t_policy, b_policy, 2, 2), \n",
    "                   [tmp_targets[0] + tmp_traces[1]*tmp_targets[1], tmp_targets[1]]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Checking whether the retrace ($\\lambda$) works in online setting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "retrace_q = retrace(policy, policy, FLenv, lambda_ = 1, step_n = 100, alpha = 0.01, gamma = 1, max_episodes = 40000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The mean difference between the theoretical and practical estimate is 0.0001\n",
      "The max difference between the theoretical and practical estimate is 0.0029\n"
     ]
    }
   ],
   "source": [
    "q_differences(q_grid_world_theoretical, retrace_q) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Checking retace ($\\lambda$) in offline setting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "a_fixed_policy = defaultdict(lambda: np.array([0.2, 0.2, 0.2, 0.4]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "retrace_q_fixed_policy = retrace(a_fixed_policy, policy, FLenv, lambda_=1, step_n=100, alpha=0.01, gamma=1, \n",
    "                                     max_episodes=40000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "theoretical_q_fixed_policy = compute_action_value_theoretically(policy=a_fixed_policy, max_episodes=2000)\n",
    "every_visit_mc_q_fixed_policy = compute_action_value(policy=a_fixed_policy, max_episodes=10000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The mean difference between the theoretical and practical estimate is 0.0059\n",
      "The max difference between the theoretical and practical estimate is 0.0762\n"
     ]
    }
   ],
   "source": [
    "q_differences(theoretical_q_fixed_policy, every_visit_mc_q_fixed_policy) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The mean difference between the theoretical and practical estimate is 0.0001\n",
      "The max difference between the theoretical and practical estimate is 0.0041\n"
     ]
    }
   ],
   "source": [
    "q_differences(retrace_q_fixed_policy, theoretical_q_fixed_policy) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The mean difference between the theoretical and practical estimate is 0.0060\n",
      "The max difference between the theoretical and practical estimate is 0.0763\n"
     ]
    }
   ],
   "source": [
    "q_differences(retrace_q_fixed_policy, every_visit_mc_q_fixed_policy) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Checking retrace ($\\lambda$) with optimal policy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Finding the optimal policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from q_learning import TabularQAgent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The algorithm reached to goal 442.0 times in 2000 number of episodes during learning phase.\n",
      "The average reward in 100 episodes is 100.0\n"
     ]
    }
   ],
   "source": [
    "q_agent = TabularQAgent(FLenv.observation_space, FLenv.action_space, init_std=0, discount=0.9, \n",
    "                        n_episodes=2000, eps=0.4)\n",
    "q_agent.learn(FLenv)\n",
    "q_agent.accuracy(FLenv, 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "optimal_policy = defaultdict(lambda: np.ones(FLenv.action_space.n, dtype=float)/FLenv.action_space.n)\n",
    "def feature_encoder(obs, state_space_n):\n",
    "    state = np.zeros((state_space_n, 1), dtype = float)\n",
    "    state[obs] = 1.0\n",
    "    return state.flatten()\n",
    "optimal_policy.update({key: feature_encoder(int(np.argmax(value)), value.shape[0]) \n",
    "                       for key, value in q_agent.q.items()})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### value of optimal policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "theoretical_q_optimal_policy = compute_action_value_theoretically(policy=optimal_policy, behavior_policy=policy,\n",
    "                                                                  max_episodes=2000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "every_visit_mc_q_optimal_policy = compute_action_value(policy=optimal_policy, max_episodes=2000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{(0, 1): 1.0,\n",
       " (4, 1): 1.0,\n",
       " (8, 2): 1.0,\n",
       " (9, 2): 1.0,\n",
       " (10, 1): 1.0,\n",
       " (14, 2): 1.0}"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "every_visit_mc_q_optimal_policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "retrace_q_optimal_policy = retrace(optimal_policy, policy, FLenv, lambda_=1, step_n=100, alpha=0.01, gamma=1, \n",
    "                                     max_episodes=40000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{(0, 1): 0.98937862863198855,\n",
       " (4, 1): 0.98942303985976499,\n",
       " (8, 2): 0.98950747505041425,\n",
       " (9, 2): 0.9900442026851487,\n",
       " (10, 1): 0.99270861749730843,\n",
       " (14, 2): 0.9958199540519876}"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "{key:retrace_q_optimal_policy[key] for key in every_visit_mc_q_optimal_policy.keys()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The mean difference between the theoretical and practical estimate is 0.0142\n",
      "The max difference between the theoretical and practical estimate is 0.0842\n"
     ]
    }
   ],
   "source": [
    "q_differences(theoretical_q_optimal_policy, retrace_q_optimal_policy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
