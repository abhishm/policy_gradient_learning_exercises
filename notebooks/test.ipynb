{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import gym\n",
    "from collections import deque\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2016-09-06 14:31:05,227] Making new env: CartPole-v0\n"
     ]
    }
   ],
   "source": [
    "env_name = 'CartPole-v0'\n",
    "env = gym.make(env_name)\n",
    "\n",
    "sess = tf.Session()\n",
    "optimizer = tf.train.RMSPropOptimizer(learning_rate=0.0001, decay=0.9)\n",
    "writer = tf.train.SummaryWriter(\"/tmp/{}-experiment-10\".format(env_name))\n",
    "\n",
    "state_dim   = env.observation_space.shape[0]\n",
    "num_actions = env.action_space.n\n",
    "discount_factor = 1\n",
    "tf_reg_param=0.001\n",
    "\n",
    "dqn_optimizer      = tf.train.RMSPropOptimizer(learning_rate=0.0001, decay=0.9)\n",
    "dqn_summary_writer = tf.train.SummaryWriter(\"/tmp/{}-experiment-1\".format(env_name))\n",
    "\n",
    "NUM_ITR = 1000\n",
    "BATCH_SIZE = 1\n",
    "MAX_STEPS    = 200\n",
    "\n",
    "episode_history = deque(maxlen=100)\n",
    "for i_itr in xrange(1):\n",
    "  episodes = []\n",
    "  for i_batch in xrange(BATCH_SIZE):\n",
    "    # initialize\n",
    "    state = env.reset()\n",
    "    total_rewards = 0\n",
    "    rewards, states, actions, next_states, returns = [], [], [], [], []\n",
    "    for t in xrange(MAX_STEPS):\n",
    "      env.render()\n",
    "      action = env.action_space.sample()# (state[np.newaxis,:])\n",
    "      next_state, reward, done, _ = env.step(action)\n",
    "      reward = -10 if done else 0.1 # normalize reward\n",
    "      ### appending the experience\n",
    "      states.append(state)\n",
    "      actions.append(action)\n",
    "      rewards.append(reward)\n",
    "      next_states.append(next_state)\n",
    "      total_rewards += reward\n",
    "\n",
    "      state = next_state\n",
    "      if done: break\n",
    "\n",
    "    return_so_far = 0\n",
    "    for reward in rewards[::-1]:\n",
    "      return_so_far = reward + discount_factor * return_so_far\n",
    "      returns.append(return_so_far)\n",
    "    #return is calculated in reverse direction\n",
    "    returns = returns[::-1]\n",
    "\n",
    "    episodes.append({\n",
    "    \"states\" : states,\n",
    "    \"actions\" : actions,\n",
    "    \"rewards\" : rewards,\n",
    "    \"returns\" : returns,\n",
    "    \"next_states\": next_states}\n",
    "    )\n",
    "\n",
    "    \n",
    "states = np.concatenate([p['states'] for p in episodes])\n",
    "next_states = np.concatenate([p['next_states'] for p in episodes])\n",
    "actions = np.concatenate([p['actions'] for p in episodes])\n",
    "returns = np.concatenate([p['returns'] for p in episodes])\n",
    "\n",
    "action_mask = np.zeros((actions.shape[0], 2))\n",
    "action_mask[np.arange(actions.shape[0]), actions] = 1\n",
    "\n",
    "next_state_mask = np.ones(states.shape[0])\n",
    "next_state_mask[-1] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class ReplayBuffer(object):\n",
    "\n",
    "  def __init__(self, buffer_size):\n",
    "\n",
    "    self.buffer_size = buffer_size\n",
    "    self.num_experiences = 0\n",
    "    self.buffer = deque()\n",
    "\n",
    "  def getBatch(self, batch_size):\n",
    "    # random draw N\n",
    "    return random.sample(self.buffer, batch_size)\n",
    "\n",
    "  def size(self):\n",
    "    return self.buffer_size\n",
    "\n",
    "  def add(self, state, action, reward, next_state, done):\n",
    "    new_experience = (state, action, reward, next_state, done)\n",
    "    if self.num_experiences < self.buffer_size:\n",
    "      self.buffer.append(new_experience)\n",
    "      self.num_experiences += 1\n",
    "    else:\n",
    "      self.buffer.popleft()\n",
    "      self.buffer.append(new_experience)\n",
    "\n",
    "  def count(self):\n",
    "    # if buffer is full, return buffer size\n",
    "    # otherwise, return experience counter\n",
    "    return self.num_experiences\n",
    "\n",
    "  def erase(self):\n",
    "    self.buffer = deque()\n",
    "    self.num_experiences = 0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def storeExperience(state, action, reward, next_state, done):\n",
    "  # always store end states\n",
    "  if dqn_store_experience_cnt % dqn_store_replay_every == 0 or done:\n",
    "    dqn_replay_buffer.add(state, action, reward, next_state, done)\n",
    "    dqn_store_experience_cnt += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def observation_to_action(states):\n",
    "  # define policy neural network\n",
    "  W1 = tf.get_variable(\"W1\", [state_dim, 20],\n",
    "                       initializer=tf.random_normal_initializer())\n",
    "  b1 = tf.get_variable(\"b1\", [20],\n",
    "                       initializer=tf.constant_initializer(0))\n",
    "  h1 = tf.nn.relu(tf.matmul(states, W1) + b1)\n",
    "  W2 = tf.get_variable(\"W2\", [20, num_actions],\n",
    "                       initializer=tf.random_normal_initializer())\n",
    "  b2 = tf.get_variable(\"b2\", [num_actions],\n",
    "                       initializer=tf.constant_initializer(0))\n",
    "  q = tf.matmul(h1, W2) + b2\n",
    "  return q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def policy_network(states):\n",
    "  # define policy neural network\n",
    "  W1 = tf.get_variable(\"W1\", [state_dim, 20],\n",
    "                       initializer=tf.random_normal_initializer())\n",
    "  b1 = tf.get_variable(\"b1\", [20],\n",
    "                       initializer=tf.constant_initializer(0))\n",
    "  h1 = tf.nn.tanh(tf.matmul(states, W1) + b1)\n",
    "  W2 = tf.get_variable(\"W2\", [20, num_actions],\n",
    "                       initializer=tf.random_normal_initializer(stddev=0.1))\n",
    "  b2 = tf.get_variable(\"b2\", [num_actions],\n",
    "                       initializer=tf.constant_initializer(0))\n",
    "  p = tf.matmul(h1, W2) + b2\n",
    "  return p"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# dqn model componenets\n",
    "dqn_q_network     = observation_to_action\n",
    "dqn_target_policy = policy_network\n",
    "dqn_replay_buffer = ReplayBuffer(buffer_size=10000)\n",
    "dqn_store_experience_cnt = 0\n",
    "dqn_batch_size = 32\n",
    "\n",
    "# training parameters\n",
    "dqn_max_gradient = 5.\n",
    "dqn_reg_param    = 0.01\n",
    "\n",
    "# counters\n",
    "dqn_store_replay_every   = 5\n",
    "dqn_store_experience_cnt = 0\n",
    "dqn_train_iteration      = 0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# compute action from a state: a* = argmax_a Q(s_t,a)\n",
    "with tf.name_scope(\"dqn_predict_actions\"):\n",
    "  # raw state representation\n",
    "  dqn_states = tf.placeholder(tf.float32, (None, state_dim), name=\"states\")\n",
    "  # initialize Q network\n",
    "  with tf.variable_scope(\"dqn_q_network\"):\n",
    "    dqn_q_outputs = dqn_q_network(dqn_states)\n",
    "  # predict actions from Q network\n",
    "  # dqn_action_scores = tf.identity(dqn_q_outputs, name=\"action_scores\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# rollout action based on current policy\n",
    "with tf.name_scope(\"policy_predict_actions\"):\n",
    "  # initialize policy network\n",
    "  with tf.variable_scope(\"policy_network\"):\n",
    "    dqn_policy_outputs = policy_network(dqn_states)\n",
    "  # predict actions from policy network\n",
    "  # dqn_action_scores = tf.identity(dqn_policy_outputs, name=\"action_scores\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# estimate rewards using the next state: r(s_t,a_t) + expectation_a Q(s_{t+1}, a)\n",
    "with tf.name_scope(\"estimate_future_rewards\"):\n",
    "  dqn_next_states = tf.placeholder(tf.float32, (None, state_dim), name=\"next_states\")\n",
    "  dqn_next_state_mask = tf.placeholder(tf.float32, (None,), name=\"next_state_masks\")\n",
    "  with tf.variable_scope(\"target_networks\"):\n",
    "      dqn_target_outputs = dqn_q_network(dqn_next_states)\n",
    "  # compute future rewards\n",
    "  next_state_rewards = tf.reduce_sum(dqn_policy_outputs * dqn_target_outputs,\n",
    "                                     reduction_indices=1, keep_dims=True)\n",
    "  next_state_rewards = tf.stop_gradient(next_state_rewards)\n",
    "  dqn_rewards = tf.placeholder(tf.float32, (None,), name=\"rewards\")\n",
    "  dqn_future_rewards = dqn_rewards + discount_factor * next_state_rewards\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dqn_q_network/W1:0\n",
      "dqn_q_network/b1:0\n",
      "dqn_q_network/W2:0\n",
      "dqn_q_network/b2:0\n",
      "policy_network/W1:0\n",
      "policy_network/b1:0\n",
      "policy_network/W2:0\n",
      "policy_network/b2:0\n",
      "target_networks/W1:0\n",
      "target_networks/b1:0\n",
      "target_networks/W2:0\n",
      "target_networks/b2:0\n"
     ]
    }
   ],
   "source": [
    "for x in tf.get_collection(\"variables\"):\n",
    "    print x.name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# compute loss and gradients\n",
    "with tf.name_scope(\"compute_temporal_differences\"):\n",
    "  # compute temporal difference loss\n",
    "  dqn_action_mask = tf.placeholder(tf.float32, (None, num_actions), name=\"action_mask\")\n",
    "  dqn_masked_action_scores = tf.reduce_sum(dqn_q_outputs * dqn_action_mask, reduction_indices=[1,])\n",
    "  dqn_temp_diff = dqn_masked_action_scores - dqn_future_rewards\n",
    "  dqn_td_loss = tf.reduce_mean(tf.square(dqn_temp_diff))\n",
    "  # regularization loss\n",
    "  q_network_variables = tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES, scope=\"q_network\")\n",
    "  dqn_reg_loss = dqn_reg_param * tf.reduce_sum([tf.reduce_sum(tf.square(x)) for x in q_network_variables])\n",
    "  # compute total loss and gradients\n",
    "  dqn_loss = dqn_td_loss + dqn_reg_loss\n",
    "  gradients = dqn_optimizer.compute_gradients(dqn_loss)\n",
    "  # clip gradients by norm\n",
    "  for i, (grad, var) in enumerate(gradients):\n",
    "    if grad is not None:\n",
    "      gradients[i] = (tf.clip_by_norm(grad, dqn_max_gradient), var)\n",
    "  # add histograms for gradients.\n",
    "#   for grad, var in gradients:\n",
    "#     tf.histogram_summary(var.name, var)\n",
    "#     if grad is not None:\n",
    "#       tf.histogram_summary(var.name + '/gradients', grad)\n",
    "  dqn_train_op = dqn_optimizer.apply_gradients(gradients)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor(\"compute_temporal_differences/clip_by_norm:0\", shape=(4, 20), dtype=float32) dqn_q_network/W1:0\n",
      "Tensor(\"compute_temporal_differences/clip_by_norm_1:0\", shape=(20,), dtype=float32) dqn_q_network/b1:0\n",
      "Tensor(\"compute_temporal_differences/clip_by_norm_2:0\", shape=(20, 2), dtype=float32) dqn_q_network/W2:0\n",
      "Tensor(\"compute_temporal_differences/clip_by_norm_3:0\", shape=(2,), dtype=float32) dqn_q_network/b2:0\n",
      "None policy_network/W1:0\n",
      "None policy_network/b1:0\n",
      "None policy_network/W2:0\n",
      "None policy_network/b2:0\n",
      "None target_networks/W1:0\n",
      "None target_networks/b1:0\n",
      "None target_networks/W2:0\n",
      "None target_networks/b2:0\n"
     ]
    }
   ],
   "source": [
    "for x in gradients:\n",
    "    try:\n",
    "        print x[0], x[1].name\n",
    "    except:\n",
    "        print x[1].name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "init_all = tf.initialize_all_variables()\n",
    "sess.run(init_all)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "sess.run(\n",
    "  dqn_train_op,\n",
    " {\n",
    "  dqn_states:          states,\n",
    "  dqn_next_states:     next_states,\n",
    "  dqn_next_state_mask: next_state_mask,\n",
    "  dqn_action_mask:     action_mask,\n",
    "  dqn_rewards:         rewards\n",
    " })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "cost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# not enough experiences yet\n",
    "if dqn_replay_buffer.count() < 32:\n",
    "      pass\n",
    "else:\n",
    "    batch           = dqn_replay_buffer.getBatch(batch_size)\n",
    "    states          = np.zeros((self.batch_size, self.state_dim))\n",
    "    rewards         = np.zeros((self.batch_size,))\n",
    "    action_mask     = np.zeros((self.batch_size, self.num_actions))\n",
    "    next_states     = np.zeros((self.batch_size, self.state_dim))\n",
    "    next_state_mask = np.zeros((self.batch_size,))\n",
    "\n",
    "    for k, (s0, a, r, s1, done) in enumerate(batch):\n",
    "      states[k] = s0\n",
    "      rewards[k] = r\n",
    "      action_mask[k][a] = 1\n",
    "      # check terminal state\n",
    "      if not done:\n",
    "        next_states[k] = s1\n",
    "        next_state_mask[k] = 1\n",
    "\n",
    "    # whether to calculate summaries\n",
    "    calculate_summaries = self.train_iteration % self.summary_every == 0 and self.summary_writer is not None\n",
    "\n",
    "    # perform one update of training\n",
    "    cost, _, summary_str = self.session.run([\n",
    "      self.loss,\n",
    "      self.train_op,\n",
    "      self.summarize if calculate_summaries else self.no_op\n",
    "    ], {\n",
    "      self.states:          states,\n",
    "      self.next_states:     next_states,\n",
    "      self.next_state_mask: next_state_mask,\n",
    "      self.action_mask:     action_mask,\n",
    "      self.rewards:         rewards\n",
    "    })\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from collections import deque"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "sess = tf.Session()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "init_var = tf.initialize_all_variables()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sess.run(init_var)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "sess.run(dqn_q_outputs, \n",
    "         feed_dict={dqn_states : states})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TensorFlow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with tf.name_scope(\"model_inputs\"):\n",
    "  # raw state representation\n",
    "  tf_states = tf.placeholder(tf.float32, (None, state_dim), name=\"states\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# rollout action based on current policy\n",
    "with tf.name_scope(\"predict_actions\"):\n",
    "  # initialize policy network\n",
    "  with tf.variable_scope(\"policy_network\"):\n",
    "    tf_policy_outputs = policy_network(tf_states)\n",
    "\n",
    "  # predict actions from policy network\n",
    "  tf_action_scores = tf.identity(tf_policy_outputs, name=\"action_scores\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# regularization loss\n",
    "policy_network_variables = tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES, scope=\"policy_network\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# compute loss and gradients\n",
    "with tf.name_scope(\"compute_pg_gradients\"):\n",
    "  # gradients for selecting action from policy network\n",
    "  tf_taken_actions = tf.placeholder(tf.int32, (None), name=\"taken_actions\")\n",
    "  tf_discounted_rewards = tf.placeholder(tf.float32, (None), name=\"discounted_rewards\")\n",
    "\n",
    "  with tf.variable_scope(\"policy_network\", reuse=True):\n",
    "    tf_logprobs = policy_network(tf_states)\n",
    "\n",
    "  # compute policy loss and regularization loss\n",
    "  tf_likelihood_loss = (tf.nn.sparse_softmax_cross_entropy_with_logits(tf_logprobs, tf_taken_actions)\n",
    "                         * tf_discounted_rewards)\n",
    "  tf_pg_loss            = tf.reduce_mean(tf_likelihood_loss)\n",
    "  tf_reg_loss           = tf.reduce_sum([tf.reduce_sum(tf.square(x)) for x in policy_network_variables])\n",
    "  tf_loss               = tf_pg_loss + tf_reg_param * tf_reg_loss\n",
    "\n",
    "  # compute gradients\n",
    "  tf_gradients = optimizer.compute_gradients(tf_loss)\n",
    "\n",
    "  # compute policy gradients\n",
    "  for i, (grad, var) in enumerate(tf_gradients):\n",
    "    if grad is not None:\n",
    "      tf_gradients[i] = (grad, var)\n",
    "\n",
    "  for grad, var in tf_gradients:\n",
    "    tf.histogram_summary(var.name, var)\n",
    "    if grad is not None:\n",
    "      tf.histogram_summary(var.name + '/gradients', grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# emit summaries\n",
    "tf.scalar_summary(\"policy_loss\", tf_pg_loss)\n",
    "tf.scalar_summary(\"reg_loss\", tf_reg_loss)\n",
    "tf.scalar_summary(\"total_loss\", tf_loss)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# training update\n",
    "with tf.name_scope(\"train_policy_network\"):\n",
    "  # apply gradients to update policy network\n",
    "  tf_train_op = optimizer.apply_gradients(tf_gradients)\n",
    "\n",
    "tf_summarize = tf.merge_all_summaries()\n",
    "tf_no_op = tf.no_op()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# evaluate gradients\n",
    "grad_evals = [grad for grad, var in tf_gradients]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "grad_evals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "init_all = tf.initialize_all_variables()\n",
    "sess.run(init_all)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "sess.run(\n",
    "    grad_evals,\n",
    "   # tf_summarize if calculate_summaries else self.no_op\n",
    "    {\n",
    "    tf_states:             states,\n",
    "    tf_taken_actions:      actions,\n",
    "    tf_discounted_rewards: returns\n",
    "    })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "[x.name for x in policy_network_variables]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "    # compute loss and gradients\n",
    "    with tf.name_scope(\"compute_pg_gradients\"):\n",
    "      # gradients for selecting action from policy network\n",
    "      self.taken_actions = tf.placeholder(tf.int32, (None, 1), name=\"taken_actions\")\n",
    "      self.discounted_rewards = tf.placeholder(tf.float32, (None, 1), name=\"discounted_rewards\")\n",
    "\n",
    "      with tf.variable_scope(\"policy_network\", reuse=True):\n",
    "        self.logprobs = self.policy_network(self.states)\n",
    "\n",
    "      # compute policy loss and regularization loss\n",
    "      self.likelihood_loss = (tf.nn.sparse_softmax_cross_entropy_with_logits(self.logprobs, self.taken_actions)\n",
    "                             * self.discounted_rewards)\n",
    "      self.pg_loss            = tf.reduce_mean(self.likelihood_loss)\n",
    "      self.reg_loss           = tf.reduce_sum([tf.reduce_sum(tf.square(x)) for x in policy_network_variables])\n",
    "      self.loss               = self.pg_loss + self.reg_param * self.reg_loss\n",
    "\n",
    "      # compute gradients\n",
    "      self.gradients = self.optimizer.compute_gradients(self.loss)\n",
    "\n",
    "      # compute policy gradients\n",
    "      for i, (grad, var) in enumerate(self.gradients):\n",
    "        if grad is not None:\n",
    "          self.gradients[i] = (grad, var)\n",
    "\n",
    "      for grad, var in self.gradients:\n",
    "        tf.histogram_summary(var.name, var)\n",
    "        if grad is not None:\n",
    "          tf.histogram_summary(var.name + '/gradients', grad)\n",
    "\n",
    "    \n",
    "    # training update\n",
    "    with tf.name_scope(\"train_policy_network\"):\n",
    "      # apply gradients to update policy network\n",
    "      self.train_op = self.optimizer.apply_gradients(self.gradients)\n",
    "\n",
    "    self.summarize = tf.merge_all_summaries()\n",
    "    self.no_op = tf.no_op()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
